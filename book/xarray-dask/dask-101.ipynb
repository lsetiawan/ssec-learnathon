{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- [Dask overview](https://tutorial.dask.org/00_overview.html)\n",
    "- [Xarray and Dask](https://tutorial.xarray.dev/intermediate/xarray_and_dask.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: What is Dask?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is a parallel and distributed computing library that scales the existing Python and PyData ecosystem.\n",
    "\n",
    "Dask can scale up to your full laptop capacity and out to a cloud cluster.\n",
    "\n",
    "There are many parts to the “Dask” the project:\n",
    "- **Collections/API** also known as “core-library”.\n",
    "- **Distributed** – to create clusters\n",
    "- **Intergrations and broader ecosystem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask provides `multi-core` and `distributed+parallel` execution on `larger-than-memory` datasets\n",
    "\n",
    "We can think of Dask’s APIs (also called collections) at a high and a low level:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://tutorial.dask.org/_images/high_vs_low_level_coll_analogy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the times when you are using Dask, you will be using a distributed scheduler, which exists in the context of a Dask cluster. The Dask cluster is structured as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://tutorial.dask.org/_images/distributed-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick demo of dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Create and connect to a local dask cluster\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "You can click on the **Dashboard** link above to open a new tab for the dask dashboard to get a full information about your workers, tasks, resources, etc.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll read [Apache Parquet](https://parquet.apache.org/) files from an [AWS S3 Bucket](https://aws.amazon.com/s3/) into a [Dask Dataframe](https://docs.dask.org/en/stable/dataframe.html) object. We’re reading the NYC taxi cab data from 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    \"s3://dask-data/nyc-taxi/nyc-2015.parquet/part.*.parquet\",\n",
    "    columns=[\"passenger_count\", \"tip_amount\"],\n",
    "    storage_options={\"anon\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{hint}\n",
    "A Dask DataFrame is a large parallel DataFrame composed of many smaller pandas DataFrames, split along the index.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform some computation below. Here we're figuring out the average tip amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ddf.groupby(\"passenger_count\").tip_amount.mean().compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "While waiting for the computation to finish, you can hop over to dask dashboard to see your computation happening in parallel in real time!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often in data processing, we're faced in having to go through a bunch of the same computation over and over again. This can be very daunting and takes a very long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a typical workflow may be an ETL(Extract-Transform-Load) pipeline:\n",
    "\n",
    "```python\n",
    "def process_file(filename):\n",
    "    data = read_a_file(filename)\n",
    "    data = do_a_transformation(data)\n",
    "    destination = f\"results/{filename}\"\n",
    "    write_out_data(data, destination)\n",
    "    return destination\n",
    "\n",
    "results = []\n",
    "for filename in filenames:\n",
    "    results.append(process_file(filename))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're processing n number of files individually in a for-loop, which means that this will take n number of time depending on the number of files since it's computing sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of computation can be improved with dask, simply by adding `@delayed` to the function:\n",
    "\n",
    "```python\n",
    "import dask\n",
    "\n",
    "@dask.delayed\n",
    "def process_file(filename):\n",
    "    # Rest of function\n",
    "    ...\n",
    "\n",
    "# For loop\n",
    "...\n",
    "\n",
    "# Now compute the results\n",
    "dask.compute(results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will now allow your processing to run in parallel (all at once if you have enough workers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://uploads.sitepoint.com/wp-content/uploads/2022/07/1658988061serial_parallel_diagram.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is seen above is called [**Dask Delayed**](https://docs.dask.org/en/latest/delayed.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dask delayed function decorates your functions so that they operate **lazily**. Rather than executing your function immediately, it will defer execution, placing the function and its arguments into a task graph.\n",
    "\n",
    "This is why you have to specifically tell dask to compute by calling `dask.compute`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example that we can execute live:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dask version:\", dask.__version__)\n",
    "print(\"Dask distributed version:\", distributed.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "@dask.delayed\n",
    "def double(x):\n",
    "    return x * 2\n",
    "\n",
    "@dask.delayed\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "output = []\n",
    "for x in data:\n",
    "    a = inc(x)\n",
    "    b = double(x)\n",
    "    c = add(a, b)\n",
    "    output.append(c)\n",
    "\n",
    "# Calculate sum... you can also wrap a function such as sum\n",
    "total = dask.delayed(sum)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See that it's a delayed function\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the task graph\n",
    "total.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute this lazy result to execute the graph in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the instance above is typically how `delayed` is used throughout various libraries. This is often the case because such routines are called CPU Bound computation, there are many calculations that happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we face a task to maybe fetch data from multiple urls. You may think that we can probably just request from all of these urls all at once using the delayed method above. Here's the catch though, some of the sites might be slower than others and so we might have to wait for that particular site, while processing the others continue.\n",
    "\n",
    "This is a common problem with I/O bound tasks. Ideally the first example of ETL should use dask futures rather than dask delayed since there are a lot of read and write happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The futures interface (derived from the built-in `concurrent.futures`) provide fine-grained real-time execution for custom situations. We can submit individual functions for evaluation with one set of inputs, or evaluated over a sequence of inputs with `submit()` and `map()`. The call returns **immediately**, giving one or more futures, whose status begins as \"pending\" and later becomes \"finished\". There is no blocking of the local Python session.\n",
    "\n",
    "This is the important difference between `futures` and `delayed`. Both can be used to support arbitrary task scheduling, but delayed is lazy (it just constructs a graph) whereas futures are eager. With futures, as soon as the inputs are available and there is compute available, the computation starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "You must start a Client to use the futures interface. This tracks state among the various worker processes or threads\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the client that has been spun up for the demo above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "URLS = ['http://www.foxnews.com/',\n",
    "        'http://www.cnn.com/',\n",
    "        'http://europe.wsj.com/',\n",
    "        'http://www.bbc.co.uk/',\n",
    "        'http://nonexistant-subdomain.python.org/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a single page and report the URL and content length\n",
    "def load_url(url, timeout):\n",
    "    try:\n",
    "        with urllib.request.urlopen(url, timeout=timeout) as conn:\n",
    "            return url, conn.headers.get('Content-Length')\n",
    "    except Exception:\n",
    "        return url, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `client.map`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`client.map` maps a function on a sequence of arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import as_completed\n",
    "\n",
    "# Create futures\n",
    "futures = client.map(load_url, URLS, timeout=60)\n",
    "\n",
    "# Get results as the request completed\n",
    "for future, result in as_completed(futures, with_results=True):\n",
    "    print(future.status, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `client.submit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`client.submit` takes a function and arguments, pushes these to the cluster, returning a `Future` representing the result to be computed. The function is passed to a worker process for evaluation. This looks a lot like doing `client.compute()`, above, except now we are passing the function and arguments directly to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "future_x = client.submit(inc, 1)\n",
    "future_y = client.submit(inc, 2)\n",
    "future_z = client.submit(sum, [future_x, future_y])\n",
    "future_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_z.result()  # waits until result is ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "The futures API offers a work submission style that can easily emulate the map/reduce paradigm. If that is familiar to you then futures might be the simplest entrypoint into Dask.\n",
    "\n",
    "The other big benefit of futures is that the intermediate results, represented by futures, can be passed to new tasks without having to pull data locally from the cluster. New operations can be setup to work on the output of previous jobs that haven’t even begun yet.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask and Xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{important}\n",
    "Using Dask does not always make your computations run faster!* \n",
    "```\n",
    "\n",
    "Performance will depend on the computational infrastructure you're using (for example, how many CPU cores), how the data you're working with is structured and stored, and the algorithms and code you're running. Be sure to review the [Dask best-practices](https://docs.dask.org/en/stable/best-practices.html) if you're new to Dask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we talk about Xarray + Dask, we are *usually* talking about two things:\n",
    "1. `dask.array` as a drop-in replacement for numpy arrays\n",
    "2. A \"scheduler\" that actually runs computations on dask arrays (commonly [distributed](https://docs.dask.org/en/stable/deploying.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array\n",
    "\n",
    "darr = dask.array.ones((10, 5), chunks=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask arrays are lazy, similar to the `delayed` functions above, so operations are not computed until you explicitely request them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darr.mean(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what did dask do when you called `.mean`? It added that operation to the \"graph\" or a blueprint of operations to execute later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(darr.mean(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we actually compute\n",
    "darr.mean(axis=-1).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray can wrap dask arrays, and we use Xarray to enable using our metadata to express our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dask-backed Xarray objects\n",
    "\n",
    "The `chunks` argument to both `open_dataset` and `open_mfdataset` allow you to\n",
    "read datasets as dask arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode minimal\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# limit the amount of information printed to screen\n",
    "xr.set_options(display_expand_data=False)\n",
    "np.set_printoptions(threshold=10, edgeitems=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's open a standard tutorial data again\n",
    "ds = xr.tutorial.open_dataset(\"air_temperature\")\n",
    "ds.air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time with dask array\n",
    "ds = xr.tutorial.open_dataset(\n",
    "    \"air_temperature\",\n",
    "    chunks={  # this tells xarray to open the dataset as a dask array\n",
    "        \"lat\": \"auto\",\n",
    "        \"lon\": 25,\n",
    "        \"time\": -1,\n",
    "    },\n",
    ")\n",
    "ds.air"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The representation (\"repr\" in Python parlance) for the `air` DataArray shows the very nice HTML dask array repr. You can access the underlying chunk sizes using `.chunks`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.air.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "All variables in a `Dataset` need _not_ have the same chunk size along\n",
    "common dimensions.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting underlying data\n",
    "\n",
    "There are two ways to pull out the underlying array object in an xarray object.\n",
    "\n",
    "1. `.to_numpy` or `.values` will always return a NumPy array. For dask-backed xarray objects,\n",
    "   this means that compute will always be called\n",
    "2. `.data` will return a Dask array\n",
    "\n",
    "```{tip}\n",
    "Use `to_numpy` or `as_numpy` instead of `.values` so that your code generalizes to other array types (like CuPy arrays, sparse arrays)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.air.data  # dask array, not numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.air.as_numpy().data  ## numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray seamlessly wraps dask so all computation is deferred until explicitly\n",
    "requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = ds.air.mean(\"time\")\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask actually constructs a graph of the required computation. Here it's pretty simple: The full array is subdivided into 3 arrays. Dask will load each of these subarrays in a separate thread using the default [single-machine scheduling](https://docs.dask.org/en/stable/scheduling.html). You can visualize dask 'task graphs' which represent the requested computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.data  # dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph for the underlying dask array\n",
    "# we ask it to visualize the graph from left to right because it looks nicer\n",
    "dask.visualize(mean.data, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting concrete values\n",
    "\n",
    "At some point, you will want to actually get concrete values (_usually_ a numpy array) from dask.\n",
    "\n",
    "There are two ways to compute values on dask arrays.\n",
    "\n",
    "1. `.compute()` returns an xarray object *just like a dask array*\n",
    "2. `.load()` replaces the dask array in the xarray object with a numpy array.\n",
    "   This is equivalent to `ds = ds.compute()`\n",
    "   \n",
    "```{tip}\n",
    "There is a third option : \"persisting\". `.persist()` loads the values into distributed RAM. The values are computed but remain distributed across workers. So `ds.air.persist()` still returns a dask array. This is useful if you will be repeatedly using a dataset for computation but it is too large to load into local memory. You will see a persistent task on the dashboard. See the [dask user guide](https://docs.dask.org/en/latest/api.html#dask.persist) for more on persisting\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributing your computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our Xarray Dataset, in addition to computing the mean, other operations such as indexing will automatically use whichever Dask Cluster we are connected to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.air.isel(lon=1, lat=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and more complicated operations...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mean = ds.air.rolling(time=5).mean()  # no activity on dashboard\n",
    "rolling_mean  # contains dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = rolling_mean.isel(lon=1, lat=20)  # no activity on dashboard\n",
    "timeseries  # contains dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = rolling_mean.compute()  # activity on dashboard\n",
    "computed  # has real numpy values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `rolling_mean` still contains a dask array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "While these operations all work, not all of them are necessarily the optimal implementation for parallelism. Usually analysis pipelines need some tinkering and tweaking to get things to work. In particular read the user guidie recommendations for [chunking](https://docs.xarray.dev/en/stable/user-guide/dask.html#chunking-and-performance) and [performance](https://docs.xarray.dev/en/stable/user-guide/dask.html#optimization-tips)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xarray data structures are first-class dask collections.\n",
    "\n",
    "This means you can do things like `dask.compute(xarray_object)`,\n",
    "`dask.visualize(xarray_object)`, `dask.persist(xarray_object)`. This works for\n",
    "both DataArrays and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish up\n",
    "Gracefully shutdown our connection to the Dask cluster. This becomes more important when you are running on large HPC or Cloud servers rather than a laptop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. See the [Xarray user guide on dask](https://docs.xarray.dev/en/stable/user-guide/dask.html).\n",
    "2. Go throught the full [dask tutorial](https://tutorial.dask.org/)\n",
    "3. Read about [Dask Gateway](https://gateway.dask.org/) to deploy and manage multi-tenant dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
